{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mulit-DOC Reading Chatbot using RAG and** **Streamlit**"
      ],
      "metadata": {
        "id": "90ydfBYMgAo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required libraries"
      ],
      "metadata": {
        "id": "qFEuA88df3D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q streamlit pymupdf chromadb pyngrok faiss-cpu sentence-transformers mistralai==0.4.2 python-docx streamlit-copy-to-clipboard"
      ],
      "metadata": {
        "id": "2t-w-IG0LwqP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required API keys and ngrok token"
      ],
      "metadata": {
        "id": "rnxNeDYXf8z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "MISTRAL_API_KEY  = userdata.get('MISTRAL_API_KEY')\n",
        "\n",
        "if NGROK_AUTH_TOKEN is None:\n",
        "    print(\"â— NGROK_AUTH_TOKEN missing in Colab secrets.\")\n",
        "if MISTRAL_API_KEY is None:\n",
        "    print(\"â— MISTRAL_API_KEY missing in Colab secrets.\")\n",
        "\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    !ngrok config add-authtoken $NGROK_AUTH_TOKEN\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFOTSD3_HtSm",
        "outputId": "8171d115-7da9-4f20-fab1-65b01aaacb0b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User-interface web-app"
      ],
      "metadata": {
        "id": "YreFbUdOgMRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os, tempfile\n",
        "import streamlit as st\n",
        "import fitz\n",
        "from docx import Document\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import chromadb\n",
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "# ---------- Constants ----------\n",
        "FALLBACK_ANSWER = \"I could not find the answer in the uploaded documents.\"\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "COLLECTION_NAME = \"doc_chunks\"\n",
        "\n",
        "# ---------- Page Configuration ----------\n",
        "st.set_page_config(page_title=\"Multi-Document Chatbot\", layout=\"wide\")\n",
        "st.title(\"ðŸ¤– Multi-Document Chatbot\")\n",
        "\n",
        "# ---------- Session State ----------\n",
        "def ensure_state():\n",
        "    ss = st.session_state\n",
        "    ss.setdefault(\"db\", None)\n",
        "    ss.setdefault(\"chunks\", [])\n",
        "    ss.setdefault(\"doc_names\", [])\n",
        "    ss.setdefault(\"history\", [])\n",
        "    ss.setdefault(\"embedder\", SentenceTransformer(EMBED_MODEL))\n",
        "    ss.setdefault(\"collection_name\", COLLECTION_NAME)\n",
        "    ss.setdefault(\"top_k\", 4)\n",
        "    ss.setdefault(\"chunk_size\", 2000)\n",
        "    ss.setdefault(\"chunk_overlap\", 400)\n",
        "    return ss\n",
        "\n",
        "ss = ensure_state()\n",
        "\n",
        "# ---------- Sidebar ----------\n",
        "with st.sidebar:\n",
        "    st.header(\"1) Upload documents\")\n",
        "    files = st.file_uploader(\n",
        "        \"PDF / DOCX / TXT\", type=[\"pdf\", \"docx\", \"txt\"],\n",
        "        accept_multiple_files=True, label_visibility=\"collapsed\"\n",
        "    )\n",
        "\n",
        "    st.header(\"2) Process\")\n",
        "    ss.top_k = st.slider(\"Top-k retrieved chunks\", 1, 12, ss.top_k, 1)\n",
        "    ss.chunk_size = st.number_input(\"Chunk size (chars)\", 500, 8000, ss.chunk_size, 100)\n",
        "    ss.chunk_overlap = st.number_input(\"Chunk overlap (chars)\", 0, 4000, ss.chunk_overlap, 50)\n",
        "\n",
        "    process_btn = st.button(\"Process Documents\", use_container_width=True)\n",
        "    clear_btn = st.button(\"Clear Chat History\", use_container_width=True)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.caption(\"API keys via environment:\")\n",
        "    st.code(\"os.environ['MISTRAL_API_KEY']\", language=\"python\")\n",
        "\n",
        "if clear_btn:\n",
        "    ss.history = []\n",
        "    st.rerun()\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "def extract_text(uploaded_file, file_type: str) -> str:\n",
        "    try:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_type}\") as tmp:\n",
        "            tmp.write(uploaded_file.getbuffer())\n",
        "            path = tmp.name\n",
        "        if file_type == \"pdf\":\n",
        "            with fitz.open(path) as doc:\n",
        "                text = \"\".join(page.get_text() for page in doc)\n",
        "        elif file_type == \"docx\":\n",
        "            d = Document(path)\n",
        "            text = \"\\n\".join([p.text for p in d.paragraphs if p.text.strip()])\n",
        "        elif file_type == \"txt\":\n",
        "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                text = f.read()\n",
        "        else:\n",
        "            text = \"\"\n",
        "        os.remove(path)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading {uploaded_file.name}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def chunk_text(text: str, size: int, overlap: int) -> List[str]:\n",
        "    step = max(size - overlap, 1)\n",
        "    return [text[i:i + size] for i in range(0, len(text), step)]\n",
        "\n",
        "# ---------- Indexing ----------\n",
        "if files and process_btn:\n",
        "    with st.spinner(\"Processing documents...\"):\n",
        "        ss.chunks, ss.doc_names = [], []\n",
        "        for f in files:\n",
        "            ext = f.name.split(\".\")[-1].lower()\n",
        "            text = extract_text(f, ext)\n",
        "            if not text:\n",
        "                continue\n",
        "            pieces = chunk_text(text, int(ss.chunk_size), int(ss.chunk_overlap))\n",
        "            ss.chunks.extend(pieces)\n",
        "            ss.doc_names.extend([f.name] * len(pieces))\n",
        "\n",
        "        if ss.chunks:\n",
        "            embedder = SentenceTransformer(EMBED_MODEL)\n",
        "            ss.embedder = embedder\n",
        "            embeddings = embedder.encode(ss.chunks, show_progress_bar=True)\n",
        "\n",
        "            client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "            if ss.collection_name in [c.name for c in client.list_collections()]:\n",
        "                client.delete_collection(ss.collection_name)\n",
        "            col = client.create_collection(ss.collection_name, metadata={\"hnsw:space\": \"cosine\"})\n",
        "            col.add(\n",
        "                documents=ss.chunks,\n",
        "                embeddings=embeddings.tolist(),\n",
        "                ids=[str(i) for i in range(len(ss.chunks))],\n",
        "                metadatas=[{\"doc\": n} for n in ss.doc_names],\n",
        "            )\n",
        "            ss.db = col\n",
        "            ss.history = []\n",
        "            st.success(f\"Processed {len(files)} document(s).\")\n",
        "\n",
        "# ---------- Chat ----------\n",
        "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
        "if ss.db is None:\n",
        "    st.info(\"Please upload and process documents to begin.\")\n",
        "elif not MISTRAL_API_KEY:\n",
        "    st.error(\"Mistral API key not found in environment. Set MISTRAL_API_KEY.\")\n",
        "else:\n",
        "    # render previous turns\n",
        "    for msg in ss.history:\n",
        "        with st.chat_message(msg[\"role\"]):\n",
        "            st.markdown(msg[\"content\"])\n",
        "            if msg[\"role\"] == \"assistant\" and msg.get(\"sources\"):\n",
        "                src = \", \".join(sorted(set(msg[\"sources\"])))\n",
        "                st.caption(f\"**Sources:** {src}\")\n",
        "\n",
        "    # input\n",
        "    if user_q := st.chat_input(\"Ask a questionâ€¦\"):\n",
        "        ss.history.append({\"role\": \"user\", \"content\": user_q})\n",
        "        st.chat_message(\"user\").write(user_q)\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Thinkingâ€¦\"):\n",
        "                q_vec = ss.embedder.encode([user_q])\n",
        "                results = ss.db.query(\n",
        "                    query_embeddings=q_vec.tolist(),\n",
        "                    n_results=int(ss.top_k),\n",
        "                    include=[\"documents\", \"metadatas\"],\n",
        "                )\n",
        "                docs = results[\"documents\"][0]\n",
        "                metas = results.get(\"metadatas\", [[{}]*ss.top_k])[0]\n",
        "                sources = [m.get(\"doc\", \"unknown\") for m in metas]\n",
        "                context = \"\\n\\n\".join(docs)\n",
        "\n",
        "                client = MistralClient(api_key=MISTRAL_API_KEY)\n",
        "                messages = [\n",
        "                    ChatMessage(\n",
        "                        role=\"system\",\n",
        "                        content=(\n",
        "                            \"You are a helpful assistant. Answer the user's question \"\n",
        "                            \"based ONLY on the provided context. If the answer is not in the \"\n",
        "                            \"context, say you could not find the answer.\\n\\n\"\n",
        "                            f\"Context:\\n{context}\"\n",
        "                        ),\n",
        "                    ),\n",
        "                    ChatMessage(role=\"user\", content=user_q),\n",
        "                ]\n",
        "                resp = client.chat(model=\"mistral-small-latest\", messages=messages)\n",
        "                answer = resp.choices[0].message.content if resp.choices else FALLBACK_ANSWER\n",
        "\n",
        "                ss.history.append({\"role\": \"assistant\", \"content\": answer, \"sources\": sources})\n",
        "                st.markdown(answer)\n",
        "                if sources:\n",
        "                    st.caption(\"**Sources:** \" + \", \".join(sorted(set(sources))))\n",
        "                st.rerun()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePHMZ0mnUCSf",
        "outputId": "d916de38-9d94-417f-cb5e-6d08e6833308"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Developer UI interface"
      ],
      "metadata": {
        "id": "r6IZxbZCoxxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_eval.py\n",
        "\n",
        "# app_eval.py\n",
        "import os, time, tempfile\n",
        "import streamlit as st\n",
        "import fitz\n",
        "from docx import Document\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import chromadb\n",
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "# ---------- Constants ----------\n",
        "FALLBACK_ANSWER = \"I could not find the answer in the uploaded documents.\"\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "COLLECTION_NAME = \"doc_chunks\"\n",
        "\n",
        "# ---------- Page ----------\n",
        "st.set_page_config(page_title=\"RAG Chatbot Evaluation\", layout=\"wide\")\n",
        "st.title(\"ðŸ”¬ RAG Chatbot Evaluation Dashboard\")\n",
        "\n",
        "# ---------- State ----------\n",
        "def ensure_state():\n",
        "    ss = st.session_state\n",
        "    ss.setdefault(\"db\", None)\n",
        "    ss.setdefault(\"chunks\", [])\n",
        "    ss.setdefault(\"doc_names\", [])\n",
        "    ss.setdefault(\"history\", [])\n",
        "    ss.setdefault(\"embedder\", SentenceTransformer(EMBED_MODEL))\n",
        "    ss.setdefault(\"collection_name\", COLLECTION_NAME)\n",
        "    ss.setdefault(\"top_k\", 4)\n",
        "    ss.setdefault(\"chunk_size\", 2000)\n",
        "    ss.setdefault(\"chunk_overlap\", 400)\n",
        "    ss.setdefault(\"grounding_threshold\", 0.55)\n",
        "    ss.setdefault(\"sim_threshold\", 0.55)\n",
        "    return ss\n",
        "\n",
        "ss = ensure_state()\n",
        "\n",
        "# ---------- Sidebar ----------\n",
        "with st.sidebar:\n",
        "    st.subheader(\"Configuration\")\n",
        "    ss.top_k = st.slider(\"Top-k\", 1, 12, ss.top_k)\n",
        "    ss.chunk_size = st.number_input(\"Chunk size\", 500, 8000, ss.chunk_size)\n",
        "    ss.chunk_overlap = st.number_input(\"Chunk overlap\", 0, 4000, ss.chunk_overlap)\n",
        "    ss.grounding_threshold = st.slider(\"Groundedness threshold\", 0.1, 0.95, ss.grounding_threshold)\n",
        "    ss.sim_threshold = st.slider(\"Relevant-chunk sim threshold\", 0.1, 0.95, ss.sim_threshold)\n",
        "\n",
        "    st.markdown(\"â€”\")\n",
        "    files = st.file_uploader(\"Upload Docs\", type=[\"pdf\",\"docx\",\"txt\"], accept_multiple_files=True)\n",
        "    process_btn = st.button(\"Process Documents\", use_container_width=True)\n",
        "\n",
        "    st.markdown(\"â€”\")\n",
        "    bench_csv = st.file_uploader(\"Optional Benchmark CSV\", type=[\"csv\"], help=\"Columns: question,expected_answer (optional: expected_doc contains filename substring)\")\n",
        "    run_bench = st.button(\"Run Benchmark (optional)\", use_container_width=True)\n",
        "\n",
        "# ---------- Utils ----------\n",
        "def extract_text(uploaded_file, file_type: str) -> str:\n",
        "    try:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_type}\") as tmp:\n",
        "            tmp.write(uploaded_file.getbuffer())\n",
        "            path = tmp.name\n",
        "        if file_type == \"pdf\":\n",
        "            with fitz.open(path) as doc:\n",
        "                text = \"\".join(page.get_text() for page in doc)\n",
        "        elif file_type == \"docx\":\n",
        "            d = Document(path)\n",
        "            text = \"\\n\".join([p.text for p in d.paragraphs if p.text.strip()])\n",
        "        elif file_type == \"txt\":\n",
        "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                text = f.read()\n",
        "        else:\n",
        "            text = \"\"\n",
        "        os.remove(path)\n",
        "        return text\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def chunk_text(text: str, size: int, overlap: int) -> List[str]:\n",
        "    step = max(size - overlap, 1)\n",
        "    return [text[i:i + size] for i in range(0, len(text), step)]\n",
        "\n",
        "def cosine_sim(a, b) -> float:\n",
        "    return float(util.cos_sim(a, b).cpu().numpy().squeeze())\n",
        "\n",
        "def timed(fn, *a, **kw):\n",
        "    t0 = time.time()\n",
        "    out = fn(*a, **kw)\n",
        "    return out, time.time() - t0\n",
        "\n",
        "def overlap_coverage(answer: str, context: str) -> float:\n",
        "    \"\"\"Coverage proxy: word overlap of answer within context.\"\"\"\n",
        "    ans_tokens = answer.lower().split()\n",
        "    ctx_tokens = context.lower().split()\n",
        "    if not ctx_tokens:\n",
        "        return 0.0\n",
        "    overlap = sum((Counter(ans_tokens) & Counter(ctx_tokens)).values())\n",
        "    return overlap / len(ans_tokens) if ans_tokens else 0.0\n",
        "\n",
        "def precision_recall_proxies(query_vec, doc_vecs: np.ndarray, sims: List[float], sim_thr: float):\n",
        "    \"\"\"Proxy: relevant if sim >= threshold (to the query).\"\"\"\n",
        "    sims_arr = np.array(sims) if len(sims) else np.array([])\n",
        "    if sims_arr.size == 0:\n",
        "        return 0.0, 0.0\n",
        "    precision = float((sims_arr >= sim_thr).sum()) / len(sims_arr)\n",
        "    recall = float(sims_arr.max())\n",
        "    return precision, recall\n",
        "\n",
        "def export_session_metrics_btn():\n",
        "    if st.button(\"ðŸ“¥ Download Session Metrics CSV\"):\n",
        "        rows = []\n",
        "        for msg in [m for m in ss.history if m[\"role\"] == \"assistant\"]:\n",
        "            r = {\"answer\": msg[\"content\"], \"is_fallback\": msg.get(\"is_fallback\", False)}\n",
        "            r.update(msg.get(\"metrics\", {}))\n",
        "            rows.append(r)\n",
        "        if not rows:\n",
        "            st.info(\"No assistant turns yet.\")\n",
        "            return\n",
        "        df = pd.DataFrame(rows)\n",
        "        csv = df.to_csv(index=False).encode(\"utf-8\")\n",
        "        st.download_button(\"Download CSV\", csv, \"session_metrics.csv\", \"text/csv\")\n",
        "\n",
        "# ---------- Indexing ----------\n",
        "if files and process_btn:\n",
        "    with st.spinner(\"Processing...\"):\n",
        "        ss.chunks, ss.doc_names = [], []\n",
        "        for f in files:\n",
        "            ext = f.name.split(\".\")[-1].lower()\n",
        "            text = extract_text(f, ext)\n",
        "            if not text:\n",
        "                continue\n",
        "            chunks = chunk_text(text, int(ss.chunk_size), int(ss.chunk_overlap))\n",
        "            ss.chunks.extend(chunks)\n",
        "            ss.doc_names.extend([f.name]*len(chunks))\n",
        "\n",
        "        if ss.chunks:\n",
        "            embeddings = ss.embedder.encode(ss.chunks, show_progress_bar=True)\n",
        "            client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "            if ss.collection_name in [c.name for c in client.list_collections()]:\n",
        "                client.delete_collection(ss.collection_name)\n",
        "            col = client.create_collection(ss.collection_name, metadata={\"hnsw:space\":\"cosine\"})\n",
        "            col.add(\n",
        "                documents=ss.chunks,\n",
        "                embeddings=embeddings.tolist(),\n",
        "                ids=[str(i) for i in range(len(ss.chunks))],\n",
        "                metadatas=[{\"doc\": n} for n in ss.doc_names],\n",
        "            )\n",
        "            ss.db = col\n",
        "            ss.history = []\n",
        "            st.success(f\"Processed {len(files)} document(s).\")\n",
        "\n",
        "# ---------- Live Dashboard ----------\n",
        "def render_dashboard():\n",
        "    st.subheader(\"ðŸ“ˆ Live Session Performance\")\n",
        "    assistant_turns = [m for m in ss.history if m[\"role\"] == \"assistant\"]\n",
        "    turn_count = len([m for m in ss.history if m[\"role\"] == \"user\"])\n",
        "    if turn_count == 0:\n",
        "        st.info(\"Start chatting below to populate metrics.\")\n",
        "        return\n",
        "\n",
        "    fallback_count = sum(1 for m in assistant_turns if m.get(\"is_fallback\"))\n",
        "    feedbacks = [m.get(\"feedback\") for m in assistant_turns if m.get(\"feedback\") is not None]\n",
        "    satisfaction_rate = (feedbacks.count(\"positive\")/len(feedbacks)*100) if feedbacks else 0.0\n",
        "    latencies = [m.get(\"metrics\",{}).get(\"total_latency_s\", 0.0) for m in assistant_turns]\n",
        "    avg_latency = np.mean(latencies) if latencies else 0.0\n",
        "\n",
        "    col1, col2, col3, col4 = st.columns(4)\n",
        "    col1.metric(\"Total Queries\", f\"{turn_count}\")\n",
        "    col2.metric(\"Avg. Latency (s)\", f\"{avg_latency:.2f}\")\n",
        "    col3.metric(\"Fallback Rate\", f\"{(fallback_count/turn_count*100):.1f}%\")\n",
        "    col4.metric(\"User Satisfaction\", f\"{satisfaction_rate:.1f}%\")\n",
        "\n",
        "render_dashboard()\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# ---------- Chat + Metrics ----------\n",
        "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
        "if ss.db and MISTRAL_API_KEY:\n",
        "    for i, msg in enumerate(ss.history):\n",
        "        with st.chat_message(msg[\"role\"]):\n",
        "            st.markdown(msg[\"content\"])\n",
        "            if msg[\"role\"] == \"assistant\":\n",
        "                m = msg.get(\"metrics\", {})\n",
        "                if m:\n",
        "                    st.markdown(\"---\")\n",
        "                    c = st.columns(6)\n",
        "                    c[0].metric(\"â±ï¸ Latency\", f\"{m.get('total_latency_s',0):.2f}s\")\n",
        "                    c[1].metric(\"ðŸ”Ž Avg Qâ€“Doc\", f\"{m.get('avg_q_doc_sim',0):.2f}\")\n",
        "                    c[2].metric(\"ðŸ… Max Qâ€“Doc\", f\"{m.get('max_q_doc_sim',0):.2f}\")\n",
        "                    c[3].metric(\"Groundedness\", f\"{m.get('answer_context_sim',0):.2f}\")\n",
        "                    c[4].metric(\"Relevance\", f\"{m.get('answer_relevance_sim',0):.2f}\")\n",
        "                    c[5].metric(\"Coverage\", f\"{m.get('coverage',0):.2f}\")\n",
        "                    cc = st.columns(3)\n",
        "                    cc[0].metric(\"Precision (proxy)\", f\"{m.get('context_precision_proxy',0):.2f}\")\n",
        "                    cc[1].metric(\"Recall (proxy)\", f\"{m.get('context_recall_proxy',0):.2f}\")\n",
        "                    cc[2].metric(\"Ans Len\", f\"{m.get('answer_length',0)}\")\n",
        "\n",
        "                if msg.get(\"hallucination_risk\"):\n",
        "                    st.warning(\"âš ï¸ Potential low grounding detected (below threshold).\")\n",
        "                with st.expander(\"Retrieved chunks & scores\"):\n",
        "                    for j, (doc, sim) in enumerate(zip(msg.get(\"retrieved_docs\",[]), msg.get(\"retrieved_sims\",[])), 1):\n",
        "                        st.markdown(f\"**Top {j} â€¢ Sim:** {sim:.3f} â€¢ **Source:** `{doc['source']}`\")\n",
        "                        st.code(doc[\"content\"][:1000] + (\"...\" if len(doc[\"content\"])>1000 else \"\"), language=\"markdown\")\n",
        "\n",
        "                fb_cols = st.columns([1,1,8])\n",
        "                if fb_cols[0].button(\"ðŸ‘\", key=f\"up_{i}\", disabled=(msg.get(\"feedback\") is not None)):\n",
        "                    msg[\"feedback\"] = \"positive\"; st.rerun()\n",
        "                if fb_cols[1].button(\"ðŸ‘Ž\", key=f\"down_{i}\", disabled=(msg.get(\"feedback\") is not None)):\n",
        "                    msg[\"feedback\"] = \"negative\"; st.rerun()\n",
        "\n",
        "    # input\n",
        "    if user_q := st.chat_input(\"Ask a questionâ€¦\"):\n",
        "        ss.history.append({\"role\": \"user\", \"content\": user_q})\n",
        "        st.chat_message(\"user\").write(user_q)\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Scoring responseâ€¦\"):\n",
        "                # Retrieval\n",
        "                q_vec, t_emb = timed(ss.embedder.encode, [user_q])\n",
        "                res, t_ret = timed(\n",
        "                    ss.db.query,\n",
        "                    query_embeddings=q_vec.tolist(),\n",
        "                    n_results=int(ss.top_k),\n",
        "                    include=[\"documents\", \"distances\", \"metadatas\", \"embeddings\"]\n",
        "                )\n",
        "                docs, dist, meta, embs = (\n",
        "                    res[\"documents\"][0],\n",
        "                    res.get(\"distances\", [[]])[0],\n",
        "                    res.get(\"metadatas\", [[]])[0],\n",
        "                    np.array(res.get(\"embeddings\", [[]])[0], dtype=np.float32) if res.get(\"embeddings\") else np.array([]),\n",
        "                )\n",
        "                sims = [1.0 - d for d in dist] if dist else []\n",
        "                context = \"\\n\\n\".join(docs)\n",
        "\n",
        "                # Generation\n",
        "                client = MistralClient(api_key=MISTRAL_API_KEY)\n",
        "                messages = [\n",
        "                    ChatMessage(role=\"system\", content=(\n",
        "                        \"Answer based ONLY on the context. If not present, say you could not find the answer.\\n\\n\"\n",
        "                        f\"Context:\\n{context}\"\n",
        "                    )),\n",
        "                    ChatMessage(role=\"user\", content=user_q),\n",
        "                ]\n",
        "                resp, t_gen = timed(client.chat, model=\"mistral-small-latest\", messages=messages)\n",
        "                answer = resp.choices[0].message.content if resp.choices else FALLBACK_ANSWER\n",
        "\n",
        "                # Embeddings for metrics\n",
        "                ans_vec = ss.embedder.encode([answer])[0]\n",
        "                ctx_vec = ss.embedder.encode([context])[0] if context.strip() else np.zeros_like(ans_vec)\n",
        "\n",
        "                if embs is not None and len(embs) > 0:\n",
        "                    avg_embs_vec = np.mean(np.array(embs), axis=0).astype(np.float32)\n",
        "                    answer_relevance_sim = cosine_sim(ans_vec, avg_embs_vec)\n",
        "                else:\n",
        "                    answer_relevance_sim = 0.0\n",
        "\n",
        "                avg_q_doc = float(np.mean(sims)) if sims else 0.0\n",
        "                max_q_doc = float(np.max(sims)) if sims else 0.0\n",
        "                groundedness = cosine_sim(ans_vec, ctx_vec) if context.strip() else 0.0\n",
        "                hallucination_score = 1.0 - groundedness\n",
        "                coverage = overlap_coverage(answer, context)\n",
        "                ans_len = len(answer.split())\n",
        "\n",
        "                # Precision/Recall (proxies) on retrieved docs\n",
        "                context_precision_proxy, context_recall_proxy = precision_recall_proxies(q_vec[0], embs, sims, float(ss.sim_threshold))\n",
        "\n",
        "                metrics = {\n",
        "                    \"total_latency_s\": float(t_emb + t_ret + t_gen),\n",
        "                    \"avg_q_doc_sim\": avg_q_doc,\n",
        "                    \"max_q_doc_sim\": max_q_doc,\n",
        "                    \"answer_context_sim\": groundedness,\n",
        "                    \"answer_relevance_sim\": answer_relevance_sim,\n",
        "                    \"hallucination_score\": hallucination_score,\n",
        "                    \"coverage\": coverage,\n",
        "                    \"answer_length\": ans_len,\n",
        "                    \"context_precision_proxy\": context_precision_proxy,\n",
        "                    \"context_recall_proxy\": context_recall_proxy,\n",
        "                }\n",
        "\n",
        "                ss.history.append({\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": answer,\n",
        "                    \"metrics\": metrics,\n",
        "                    \"is_fallback\": (answer == FALLBACK_ANSWER),\n",
        "                    \"feedback\": None,\n",
        "                    \"hallucination_risk\": (groundedness < float(ss.grounding_threshold)),\n",
        "                    \"retrieved_docs\": [{\"content\": d, \"source\": m.get(\"doc\")} for d, m in zip(docs, meta)],\n",
        "                    \"retrieved_sims\": sims,\n",
        "                })\n",
        "                st.markdown(answer)\n",
        "                st.rerun()\n",
        "else:\n",
        "    if ss.db is None:\n",
        "        st.info(\"Please upload and process documents.\")\n",
        "    elif not MISTRAL_API_KEY:\n",
        "        st.error(\"MISTRAL_API_KEY not found in environment.\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "export_session_metrics_btn()\n",
        "\n",
        "# ---------- Optional: Benchmark runner ----------\n",
        "if ss.db and MISTRAL_API_KEY and bench_csv is not None and run_bench:\n",
        "    df = pd.read_csv(bench_csv)\n",
        "    if \"question\" not in df.columns:\n",
        "        st.error(\"CSV must have a 'question' column.\")\n",
        "    else:\n",
        "        st.info(\"Running benchmarkâ€¦ This will iterate through rows and compute metrics.\")\n",
        "        results = []\n",
        "        client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "        col = client.get_collection(ss.collection_name)\n",
        "\n",
        "        mistral = MistralClient(api_key=MISTRAL_API_KEY)\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            q = str(row[\"question\"])\n",
        "            gt = str(row.get(\"expected_answer\", \"\")) if \"expected_answer\" in df.columns else \"\"\n",
        "            expected_doc = str(row.get(\"expected_doc\", \"\")) if \"expected_doc\" in df.columns else \"\"\n",
        "\n",
        "            # Retrieval\n",
        "            q_vec = ss.embedder.encode([q])\n",
        "            res = col.query(\n",
        "                query_embeddings=q_vec.tolist(),\n",
        "                n_results=int(ss.top_k),\n",
        "                include=[\"documents\",\"distances\",\"metadatas\",\"embeddings\"]\n",
        "            )\n",
        "            docs = res[\"documents\"][0]\n",
        "            dist = res.get(\"distances\",[[]])[0]\n",
        "            meta = res.get(\"metadatas\",[[]])[0]\n",
        "            sims = [1.0 - d for d in dist] if dist else []\n",
        "            context = \"\\n\\n\".join(docs)\n",
        "\n",
        "            # Generation\n",
        "            messages = [\n",
        "                ChatMessage(role=\"system\", content=(\n",
        "                    \"Answer based ONLY on the context. If not present, say you could not find the answer.\\n\\n\"\n",
        "                    f\"Context:\\n{context}\"\n",
        "                )),\n",
        "                ChatMessage(role=\"user\", content=q),\n",
        "            ]\n",
        "            resp = mistral.chat(model=\"mistral-small-latest\", messages=messages)\n",
        "            ans = resp.choices[0].message.content if resp.choices else FALLBACK_ANSWER\n",
        "\n",
        "            # Metrics\n",
        "            ans_vec = ss.embedder.encode([ans])[0]\n",
        "            ctx_vec = ss.embedder.encode([context])[0] if context.strip() else np.zeros_like(ans_vec)\n",
        "            groundedness = cosine_sim(ans_vec, ctx_vec) if context.strip() else 0.0\n",
        "\n",
        "            # Answer relevance vs retrieved embeddings centroid\n",
        "            embs = np.array(res.get(\"embeddings\",[[]])[0], dtype=np.float32) if res.get(\"embeddings\") else np.array([])\n",
        "            if embs.size > 0:\n",
        "                answer_relevance_sim = cosine_sim(ans_vec, np.mean(embs, axis=0).astype(np.float32))\n",
        "            else:\n",
        "                answer_relevance_sim = 0.0\n",
        "\n",
        "            coverage = overlap_coverage(ans, context)\n",
        "            avg_q_doc = float(np.mean(sims)) if sims else 0.0\n",
        "            max_q_doc = float(np.max(sims)) if sims else 0.0\n",
        "            precision_proxy, recall_proxy = precision_recall_proxies(q_vec[0], embs, sims, float(ss.sim_threshold))\n",
        "\n",
        "            # Optional semantic accuracy vs ground-truth\n",
        "            if gt.strip():\n",
        "                gt_vec = ss.embedder.encode([gt])[0]\n",
        "                semantic_accuracy = (cosine_sim(ans_vec, gt_vec) + 1) / 2  # map [-1,1] -> [0,1]\n",
        "            else:\n",
        "                semantic_accuracy = np.nan\n",
        "\n",
        "            # Optional retrieval recall@k using expected_doc matching\n",
        "            if expected_doc and meta:\n",
        "                retrieved_names = [m.get(\"doc\",\"\") for m in meta]\n",
        "                recall_at_k = 1.0 if any(expected_doc.lower() in (n or \"\").lower() for n in retrieved_names) else 0.0\n",
        "            else:\n",
        "                recall_at_k = np.nan\n",
        "\n",
        "            results.append({\n",
        "                \"question\": q,\n",
        "                \"answer\": ans,\n",
        "                \"avg_q_doc_sim\": avg_q_doc,\n",
        "                \"max_q_doc_sim\": max_q_doc,\n",
        "                \"answer_context_sim\": groundedness,\n",
        "                \"answer_relevance_sim\": answer_relevance_sim,\n",
        "                \"coverage\": coverage,\n",
        "                \"context_precision_proxy\": precision_proxy,\n",
        "                \"context_recall_proxy\": recall_proxy,\n",
        "                \"semantic_accuracy_vs_gt\": semantic_accuracy,\n",
        "                \"retrieval_recall_at_k_label\": recall_at_k,\n",
        "            })\n",
        "\n",
        "        bench_df = pd.DataFrame(results)\n",
        "        st.success(\"Benchmark finished.\")\n",
        "        st.dataframe(bench_df.head(20))\n",
        "        st.download_button(\n",
        "            \"ðŸ“¥ Download Benchmark Results\",\n",
        "            bench_df.to_csv(index=False).encode(\"utf-8\"),\n",
        "            \"benchmark_results.csv\",\n",
        "            \"text/csv\"\n",
        "        )\n",
        "\n",
        "        st.markdown(\"### ðŸ“„Summary\")\n",
        "        summary = {\n",
        "            \"Avg Retrieval (Qâ€“Doc)\": bench_df[\"avg_q_doc_sim\"].mean(),\n",
        "            \"Avg Groundedness\": bench_df[\"answer_context_sim\"].mean(),\n",
        "            \"Avg Relevance\": bench_df[\"answer_relevance_sim\"].mean(),\n",
        "            \"Avg Coverage\": bench_df[\"coverage\"].mean(),\n",
        "            \"Precision Proxy\": bench_df[\"context_precision_proxy\"].mean(),\n",
        "            \"Recall Proxy\": bench_df[\"context_recall_proxy\"].mean(),\n",
        "            \"Semantic Accuracy (if GT)\": bench_df[\"semantic_accuracy_vs_gt\"].mean(skipna=True),\n",
        "            \"Recall@k (if label)\": bench_df[\"retrieval_recall_at_k_label\"].mean(skipna=True),\n",
        "        }\n",
        "        st.json({k: (float(v) if pd.notna(v) else None) for k, v in summary.items()})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qByA_tnaBRAQ",
        "outputId": "648fb0dd-b0c1-4d49-d71e-a2943171387c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_eval.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZKglzc2sAbF",
        "outputId": "6e87c8fa-283b-48a9-87df-cdd30f593ad9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 36\n",
            "-rw-r--r--  1 root root 18585 Aug 21 12:41 app_eval.py\n",
            "-rw-r--r--  1 root root  6780 Aug 21 12:41 app.py\n",
            "drwxr-xr-x 10 root root  4096 Aug 21 12:29 chroma_db\n",
            "drwxr-xr-x  1 root root  4096 Aug 19 13:38 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit Code for both user and developer"
      ],
      "metadata": {
        "id": "koh3XOolpYMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f streamlit || echo \"No old Streamlit process\"\n",
        "!pkill -f ngrok || echo \"No old ngrok process\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gApT6bImgxZT",
        "outputId": "1406b778-fa83-45fa-908a-bb7e19ddd302"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, subprocess\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "# Secrets -> env\n",
        "os.environ['MISTRAL_API_KEY'] = userdata.get('MISTRAL_API_KEY') or \"\"\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN') or \"\"\n",
        "\n",
        "if not os.environ['MISTRAL_API_KEY']:\n",
        "    print(\"â— MISTRAL_API_KEY missing in Colab secrets.\")\n",
        "if not NGROK_AUTH_TOKEN:\n",
        "    print(\"â— NGROK_AUTH_TOKEN missing in Colab secrets.\")\n",
        "\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "APP_FILE = \"app_eval.py\"\n",
        "\n",
        "# Launch Streamlit\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", APP_FILE, \"--server.port=8501\"])\n",
        "print(\"Starting Streamlit server...\")\n",
        "time.sleep(5)\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"âœ… {APP_FILE} is live at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhIGplZtgmdW",
        "outputId": "610ea537-f451-4e31-9e90-4cde7da86da7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Streamlit server...\n",
            "âœ… app_eval.py is live at: NgrokTunnel: \"https://07d7311c9a33.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, subprocess\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "# Secrets -> env\n",
        "os.environ['MISTRAL_API_KEY'] = userdata.get('MISTRAL_API_KEY') or \"\"\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN') or \"\"\n",
        "\n",
        "if not os.environ['MISTRAL_API_KEY']:\n",
        "    print(\"â— MISTRAL_API_KEY missing in Colab secrets.\")\n",
        "if not NGROK_AUTH_TOKEN:\n",
        "    print(\"â— NGROK_AUTH_TOKEN missing in Colab secrets.\")\n",
        "\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "APP_FILE = \"app.py\"\n",
        "\n",
        "# Launch Streamlit\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", APP_FILE, \"--server.port=8501\"])\n",
        "print(\"Starting Streamlit server...\")\n",
        "time.sleep(5)\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"âœ… {APP_FILE} is live at: {public_url}\")"
      ],
      "metadata": {
        "id": "DXxlR9gdt5sG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f9c78f-0283-4129-cbe7-1f6f4cc761fe"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Streamlit server...\n",
            "âœ… app.py is live at: NgrokTunnel: \"https://a1bcfaa2e0e7.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gwf-Gf2JDfP8"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}