{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90ydfBYMgAo0"
      },
      "source": [
        "# **Mulit-DOC Reading Chatbot using RAG and** **Streamlit**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFEuA88df3D_"
      },
      "source": [
        "## Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2t-w-IG0LwqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e8b2fc-79a1-4b79-b9c4-3f181811f0dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q streamlit pymupdf chromadb pyngrok faiss-cpu sentence-transformers mistralai==0.4.2 python-docx streamlit-copy-to-clipboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain\n",
        "!pip install -U langchain-community\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7ScQTnXO076",
        "outputId": "9dd54f8e-1ace-4f2c-f10b-0e67af4f73b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.27)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.75)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Collecting requests<3,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.27)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-community-0.3.29 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo83WXErA0sj",
        "outputId": "799fa050-0c53-4cc5-964a-e18a64d7add2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: streamlit\n",
            "Version: 1.49.1\n",
            "Summary: A faster way to build and share data apps\n",
            "Home-page: https://streamlit.io\n",
            "Author: Snowflake Inc\n",
            "Author-email: hello@streamlit.io\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: altair, blinker, cachetools, click, gitpython, numpy, packaging, pandas, pillow, protobuf, pyarrow, pydeck, requests, tenacity, toml, tornado, typing-extensions, watchdog\n",
            "Required-by: streamlit-copy-to-clipboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnxNeDYXf8z1"
      },
      "source": [
        "## Required API keys and ngrok token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFOTSD3_HtSm",
        "outputId": "d8e7e2e6-adb7-46f1-eeae-a46691516cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "MISTRAL_API_KEY  = userdata.get('MISTRAL_API_KEY')\n",
        "\n",
        "if NGROK_AUTH_TOKEN is None:\n",
        "    print(\"❗ NGROK_AUTH_TOKEN missing in Colab secrets.\")\n",
        "if MISTRAL_API_KEY is None:\n",
        "    print(\"❗ MISTRAL_API_KEY missing in Colab secrets.\")\n",
        "\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    !ngrok config add-authtoken $NGROK_AUTH_TOKEN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YreFbUdOgMRq"
      },
      "source": [
        "## User-interface web-app"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os, tempfile\n",
        "import streamlit as st\n",
        "import fitz\n",
        "from docx import Document\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import chromadb\n",
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "# ---------- Constants ----------\n",
        "FALLBACK_ANSWER = \"I could not find the answer in the uploaded documents.\"\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "COLLECTION_NAME = \"doc_chunks\"\n",
        "\n",
        "# ---------- Page Config ----------\n",
        "st.set_page_config(page_title=\"Multi-Document Chatbot with Dashboard\", layout=\"wide\")\n",
        "st.title(\"🤖 Multi-Document Chatbot with Dashboard\")\n",
        "\n",
        "# ---------- Session State Initialization ----------\n",
        "def ensure_state():\n",
        "    ss = st.session_state\n",
        "    ss.setdefault(\"db\", None)\n",
        "    ss.setdefault(\"chunks\", [])\n",
        "    ss.setdefault(\"doc_names\", [])\n",
        "    ss.setdefault(\"history\", [])\n",
        "    ss.setdefault(\"answered_questions\", set())\n",
        "    ss.setdefault(\"embedder\", SentenceTransformer(EMBED_MODEL))\n",
        "    ss.setdefault(\"collection_name\", COLLECTION_NAME)\n",
        "    ss.setdefault(\"top_k\", 4)\n",
        "    ss.setdefault(\"chunk_size\", 2000)\n",
        "    ss.setdefault(\"chunk_overlap\", 400)\n",
        "    ss.setdefault(\"grounding_threshold\", 0.55)\n",
        "    ss.setdefault(\"sim_threshold\", 0.55)\n",
        "    return ss\n",
        "\n",
        "ss = ensure_state()\n",
        "\n",
        "# ---------- Sidebar ----------\n",
        "with st.sidebar:\n",
        "    st.header(\"1) Upload documents\")\n",
        "    files = st.file_uploader(\n",
        "        \"PDF / DOCX / TXT\", type=[\"pdf\", \"docx\", \"txt\"],\n",
        "        accept_multiple_files=True, label_visibility=\"collapsed\"\n",
        "    )\n",
        "\n",
        "    st.header(\"2) Processing Parameters\")\n",
        "    ss.top_k = st.slider(\"Top-k retrieved chunks\", 1, 12, ss.top_k, 1)\n",
        "    ss.chunk_size = st.number_input(\"Chunk size (chars)\", 500, 8000, ss.chunk_size, 100)\n",
        "    ss.chunk_overlap = st.number_input(\"Chunk overlap (chars)\", 0, 4000, ss.chunk_overlap, 50)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.header(\"3) RAG/Performance Metrics\")\n",
        "    ss.grounding_threshold = st.slider(\"Groundedness threshold\", 0.1, 0.95, ss.grounding_threshold)\n",
        "    ss.sim_threshold = st.slider(\"Relevant-chunk sim threshold\", 0.1, 0.95, ss.sim_threshold)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    process_btn = st.button(\"Process Documents\", use_container_width=True)\n",
        "    clear_btn = st.button(\"Clear Chat History\", use_container_width=True)\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.caption(\"API keys via environment variables (set MISTRAL_API_KEY)\")\n",
        "    st.code(\"os.environ['MISTRAL_API_KEY']\", language=\"python\")\n",
        "\n",
        "if clear_btn:\n",
        "    ss.history = []\n",
        "    ss.answered_questions = set()\n",
        "    st.experimental_rerun()\n",
        "\n",
        "# ---------- Utility Functions ----------\n",
        "def extract_text(uploaded_file, file_type: str) -> str:\n",
        "    try:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_type}\") as tmp:\n",
        "            tmp.write(uploaded_file.getbuffer())\n",
        "            path = tmp.name\n",
        "        if file_type == \"pdf\":\n",
        "            with fitz.open(path) as doc:\n",
        "                text = \"\".join(page.get_text() for page in doc)\n",
        "        elif file_type == \"docx\":\n",
        "            d = Document(path)\n",
        "            text = \"\\n\".join([p.text for p in d.paragraphs if p.text.strip()])\n",
        "        elif file_type == \"txt\":\n",
        "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                text = f.read()\n",
        "        else:\n",
        "            text = \"\"\n",
        "        os.remove(path)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading {uploaded_file.name}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def chunk_text(text: str, size: int, overlap: int) -> List[str]:\n",
        "    step = max(size - overlap, 1)\n",
        "    return [text[i:i + size] for i in range(0, len(text), step)]\n",
        "\n",
        "def cosine_sim(a, b) -> float:\n",
        "    return float(util.cos_sim(a, b).cpu().numpy().squeeze())\n",
        "\n",
        "def overlap_coverage(answer: str, context: str) -> float:\n",
        "    ans_tokens = answer.lower().split()\n",
        "    ctx_tokens = context.lower().split()\n",
        "    if not ctx_tokens:\n",
        "        return 0.0\n",
        "    overlap = sum((Counter(ans_tokens) & Counter(ctx_tokens)).values())\n",
        "    return overlap / len(ans_tokens) if ans_tokens else 0.0\n",
        "\n",
        "def precision_recall_proxies(query_vec, doc_vecs: np.ndarray, sims: List[float], sim_thr: float):\n",
        "    sims_arr = np.array(sims) if len(sims) else np.array([])\n",
        "    if sims_arr.size == 0:\n",
        "        return 0.0, 0.0\n",
        "    precision = float((sims_arr >= sim_thr).sum()) / len(sims_arr)\n",
        "    recall = float(sims_arr.max())\n",
        "    return precision, recall\n",
        "\n",
        "def timed(fn, *a, **kw):\n",
        "    import time\n",
        "    t0 = time.time()\n",
        "    out = fn(*a, **kw)\n",
        "    return out, time.time() - t0\n",
        "\n",
        "# ---------- Document Processing ----------\n",
        "if files and process_btn:\n",
        "    with st.spinner(\"Processing documents...\"):\n",
        "        ss.chunks, ss.doc_names = [], []\n",
        "        for f in files:\n",
        "            ext = f.name.split(\".\")[-1].lower()\n",
        "            text = extract_text(f, ext)\n",
        "            if not text:\n",
        "                continue\n",
        "            pieces = chunk_text(text, int(ss.chunk_size), int(ss.chunk_overlap))\n",
        "            ss.chunks.extend(pieces)\n",
        "            ss.doc_names.extend([f.name]*len(pieces))\n",
        "\n",
        "        if ss.chunks:\n",
        "            embedder = SentenceTransformer(EMBED_MODEL)\n",
        "            ss.embedder = embedder\n",
        "            embeddings = embedder.encode(ss.chunks, show_progress_bar=True)\n",
        "\n",
        "            client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "            if ss.collection_name in [c.name for c in client.list_collections()]:\n",
        "                client.delete_collection(ss.collection_name)\n",
        "            col = client.create_collection(ss.collection_name, metadata={\"hnsw:space\":\"cosine\"})\n",
        "            col.add(\n",
        "                documents=ss.chunks,\n",
        "                embeddings=embeddings.tolist(),\n",
        "                ids=[str(i) for i in range(len(ss.chunks))],\n",
        "                metadatas=[{\"doc\": n} for n in ss.doc_names],\n",
        "            )\n",
        "            ss.db = col\n",
        "            ss.history = []\n",
        "            ss.answered_questions = set()\n",
        "            st.success(f\"Processed {len(files)} document(s).\")\n",
        "\n",
        "# ---------- Main Interface Tabs ----------\n",
        "tabs = st.tabs([\"Chat\", \"Dashboard\"])\n",
        "\n",
        "# ---------- Chat Tab ----------\n",
        "with tabs[0]:\n",
        "    MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
        "\n",
        "    if ss.db is None:\n",
        "        st.info(\"Please upload and process documents to begin.\")\n",
        "    elif not MISTRAL_API_KEY:\n",
        "        st.error(\"Mistral API key not found in environment. Set MISTRAL_API_KEY.\")\n",
        "    else:\n",
        "        # Display chat history (user + assistant messages, single loop)\n",
        "        for i, msg in enumerate(ss.history):\n",
        "            with st.chat_message(msg[\"role\"]):\n",
        "                st.markdown(msg[\"content\"])\n",
        "                if msg[\"role\"] == \"assistant\" and msg.get(\"metrics\"):\n",
        "                    m = msg.get(\"metrics\")\n",
        "                    cols = st.columns(6)\n",
        "                    cols[0].metric(\"⏱️ Latency\", f\"{m.get('total_latency_s', 0):.2f}s\")\n",
        "                    cols[1].metric(\"🔎 Avg Q–Doc\", f\"{m.get('avg_q_doc_sim', 0):.2f}\")\n",
        "                    cols[2].metric(\"🏅 Max Q–Doc\", f\"{m.get('max_q_doc_sim', 0):.2f}\")\n",
        "                    cols[3].metric(\"Groundedness\", f\"{m.get('answer_context_sim', 0):.2f}\")\n",
        "                    cols[4].metric(\"Relevance\", f\"{m.get('answer_relevance_sim', 0):.2f}\")\n",
        "                    cols[5].metric(\"Coverage\", f\"{m.get('coverage', 0):.2f}\")\n",
        "\n",
        "                    fb_cols = st.columns([1,1,8])\n",
        "                    if fb_cols[0].button(\"👍\", key=f\"up_{i}\", disabled=(msg.get(\"feedback\") is not None)):\n",
        "                        msg[\"feedback\"] = \"positive\"\n",
        "                        # optionally add a flag to trigger rerun but avoid duplicate appends\n",
        "                    if fb_cols[1].button(\"👎\", key=f\"down_{i}\", disabled=(msg.get(\"feedback\") is not None)):\n",
        "                        msg[\"feedback\"] = \"negative\"\n",
        "\n",
        "                    if msg.get(\"hallucination_risk\"):\n",
        "                        st.warning(\"⚠️ Potential low grounding detected (below threshold).\")\n",
        "\n",
        "                    with st.expander(\"Retrieved chunks & scores\"):\n",
        "                        for j, (doc, sim) in enumerate(zip(msg.get(\"retrieved_docs\", []), msg.get(\"retrieved_sims\", [])), 1):\n",
        "                            st.markdown(f\"**Top {j} • Sim:** {sim:.3f} • **Source:** `{doc['source']}`\")\n",
        "                            st.code(doc[\"content\"][:1000] + (\"...\" if len(doc[\"content\"]) > 1000 else \"\"), language=\"markdown\")\n",
        "\n",
        "        # Chat input at bottom\n",
        "        user_input = st.chat_input(\"Ask a question...\", key=\"chat_input\")\n",
        "\n",
        "        if user_input and user_input not in ss.answered_questions:\n",
        "            ss.answered_questions.add(user_input)\n",
        "            ss.history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    # Retrieval\n",
        "                    q_vec, t_emb = timed(ss.embedder.encode, [user_input])\n",
        "                    res, t_ret = timed(\n",
        "                        ss.db.query,\n",
        "                        query_embeddings=q_vec.tolist(),\n",
        "                        n_results=int(ss.top_k),\n",
        "                        include=[\"documents\", \"distances\", \"metadatas\", \"embeddings\"]\n",
        "                    )\n",
        "                    docs, dist, meta, embs = (\n",
        "                        res[\"documents\"][0],\n",
        "                        res.get(\"distances\", [[]])[0],\n",
        "                        res.get(\"metadatas\", [[]])[0],\n",
        "                        np.array(res.get(\"embeddings\", [[]])[0], dtype=np.float32) if res.get(\"embeddings\") else np.array([]),\n",
        "                    )\n",
        "                    sims = [1.0 - d for d in dist] if dist else []\n",
        "                    context = \"\\n\\n\".join(docs)\n",
        "\n",
        "                    client = MistralClient(api_key=MISTRAL_API_KEY)\n",
        "                    messages = [\n",
        "                        ChatMessage(role=\"system\", content=(\n",
        "                            \"Answer based ONLY on the context. If not present, say you could not find the answer.\\n\\n\"\n",
        "                            f\"Context:\\n{context}\"\n",
        "                        )),\n",
        "                        ChatMessage(role=\"user\", content=user_input),\n",
        "                    ]\n",
        "                    resp, t_gen = timed(client.chat, model=\"mistral-small-latest\", messages=messages)\n",
        "                    answer = resp.choices[0].message.content if resp.choices else FALLBACK_ANSWER\n",
        "\n",
        "                    ans_vec = ss.embedder.encode([answer])[0]\n",
        "                    ctx_vec = ss.embedder.encode([context])[0] if context.strip() else np.zeros_like(ans_vec)\n",
        "\n",
        "                    if embs is not None and len(embs) > 0:\n",
        "                        avg_embs_vec = np.mean(np.array(embs), axis=0).astype(np.float32)\n",
        "                        answer_relevance_sim = cosine_sim(ans_vec, avg_embs_vec)\n",
        "                    else:\n",
        "                        answer_relevance_sim = 0.0\n",
        "\n",
        "                    avg_q_doc = float(np.mean(sims)) if sims else 0.0\n",
        "                    max_q_doc = float(np.max(sims)) if sims else 0.0\n",
        "                    groundedness = cosine_sim(ans_vec, ctx_vec) if context.strip() else 0.0\n",
        "                    hallucination_score = 1.0 - groundedness\n",
        "                    coverage = overlap_coverage(answer, context)\n",
        "                    ans_len = len(answer.split())\n",
        "\n",
        "                    context_precision_proxy, context_recall_proxy = precision_recall_proxies(q_vec[0], embs, sims, float(ss.sim_threshold))\n",
        "\n",
        "                    metrics = {\n",
        "                        \"total_latency_s\": float(t_emb + t_ret + t_gen),\n",
        "                        \"avg_q_doc_sim\": avg_q_doc,\n",
        "                        \"max_q_doc_sim\": max_q_doc,\n",
        "                        \"answer_context_sim\": groundedness,\n",
        "                        \"answer_relevance_sim\": answer_relevance_sim,\n",
        "                        \"hallucination_score\": hallucination_score,\n",
        "                        \"coverage\": coverage,\n",
        "                        \"answer_length\": ans_len,\n",
        "                        \"context_precision_proxy\": context_precision_proxy,\n",
        "                        \"context_recall_proxy\": context_recall_proxy,\n",
        "                    }\n",
        "\n",
        "                    ss.history.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": answer,\n",
        "                        \"metrics\": metrics,\n",
        "                        \"is_fallback\": (answer == FALLBACK_ANSWER),\n",
        "                        \"feedback\": None,\n",
        "                        \"hallucination_risk\": (groundedness < float(ss.grounding_threshold)),\n",
        "                        \"retrieved_docs\": [{\"content\": d, \"source\": m.get(\"doc\")} for d, m in zip(docs, meta)],\n",
        "                        \"retrieved_sims\": sims,\n",
        "                    })\n",
        "\n",
        "                    st.markdown(answer)\n",
        "\n",
        "# ---------- Dashboard Tab ----------\n",
        "with tabs[1]:\n",
        "    st.subheader(\"📈 Live Session Performance\")\n",
        "    assistant_turns = [m for m in ss.history if m[\"role\"] == \"assistant\"]\n",
        "    turn_count = len([m for m in ss.history if m[\"role\"] == \"user\"])\n",
        "    if turn_count == 0:\n",
        "        st.info(\"Start chatting in the Chat tab to populate metrics.\")\n",
        "    else:\n",
        "        fallback_count = sum(1 for m in assistant_turns if m.get(\"is_fallback\"))\n",
        "        feedbacks = [m.get(\"feedback\") for m in assistant_turns if m.get(\"feedback\") is not None]\n",
        "        satisfaction_rate = (feedbacks.count(\"positive\") / len(feedbacks) * 100) if feedbacks else 0.0\n",
        "        latencies = [m.get(\"metrics\", {}).get(\"total_latency_s\", 0.0) for m in assistant_turns]\n",
        "        avg_latency = np.mean(latencies) if latencies else 0.0\n",
        "\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "        col1.metric(\"Total Queries\", f\"{turn_count}\")\n",
        "        col2.metric(\"Avg. Latency (s)\", f\"{avg_latency:.2f}\")\n",
        "        col3.metric(\"Fallback Rate\", f\"{(fallback_count / turn_count * 100):.1f}%\")\n",
        "        col4.metric(\"User Satisfaction\", f\"{satisfaction_rate:.1f}%\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "# ---------- Export Session Metrics ----------\n",
        "def export_session_metrics_btn():\n",
        "    if st.button(\"📥 Download Session Metrics CSV\"):\n",
        "        rows = []\n",
        "        for msg in [m for m in ss.history if m[\"role\"] == \"assistant\"]:\n",
        "            r = {\"answer\": msg[\"content\"], \"is_fallback\": msg.get(\"is_fallback\", False)}\n",
        "            r.update(msg.get(\"metrics\", {}))\n",
        "            rows.append(r)\n",
        "        if not rows:\n",
        "            st.info(\"No assistant turns yet.\")\n",
        "            return\n",
        "        df = pd.DataFrame(rows)\n",
        "        csv = df.to_csv(index=False).encode(\"utf-8\")\n",
        "        st.download_button(\"Download CSV\", csv, \"session_metrics.csv\", \"text/csv\")\n",
        "\n",
        "export_session_metrics_btn()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNroaziIMCr9",
        "outputId": "e436cd3e-958a-4df9-c36c-79277a441c44"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZKglzc2sAbF",
        "outputId": "7ef1a0ff-ae22-47bf-983c-0c3f391442e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 24\n",
            "-rw-r--r-- 1 root root 16488 Sep 18 05:07 app.py\n",
            "drwxr-xr-x 1 root root  4096 Sep 16 13:40 sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koh3XOolpYMS"
      },
      "source": [
        "## Streamlit Code for both user and developer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gApT6bImgxZT",
        "outputId": "66728940-8452-4a15-bbff-0419f01ed855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pkill -f streamlit || echo \"No old Streamlit process\"\n",
        "!pkill -f ngrok || echo \"No old ngrok process\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXxlR9gdt5sG",
        "outputId": "eb665fb9-8f52-4bf0-93a7-e29be700a0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Streamlit server...\n",
            "✅ app.py is live at: NgrokTunnel: \"https://c64695acba98.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "import os, time, subprocess\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "# Secrets -> env\n",
        "os.environ['MISTRAL_API_KEY'] = userdata.get('MISTRAL_API_KEY') or \"\"\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN') or \"\"\n",
        "\n",
        "if not os.environ['MISTRAL_API_KEY']:\n",
        "    print(\"❗ MISTRAL_API_KEY missing in Colab secrets.\")\n",
        "if not NGROK_AUTH_TOKEN:\n",
        "    print(\"❗ NGROK_AUTH_TOKEN missing in Colab secrets.\")\n",
        "\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "APP_FILE = \"app.py\"\n",
        "\n",
        "# Launch Streamlit\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", APP_FILE, \"--server.port=8501\"])\n",
        "print(\"Starting Streamlit server...\")\n",
        "time.sleep(5)\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"✅ {APP_FILE} is live at: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwf-Gf2JDfP8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}